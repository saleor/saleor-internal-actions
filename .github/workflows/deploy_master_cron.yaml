name: Build and deploy master staging
on:
  schedule:
    # Every workday (MON-FRI) at 10am and 4pm UTC
    - cron: "0 10,16 * * 1-5"
  workflow_dispatch:
jobs:
  build:
    runs-on: ubuntu-20.04

    env:
      DEPLOY_CLUSTER_NAME: saleor-cloud
      STAGING_K8S_NAMESPACE: cloud-staging
      MASTER_DEPLOYMENT_NAME: saleor-master-staging
      AWS_CD_ROLE_ARN: arn:aws:iam::727618530289:role/CD_edit_k8s_cloud_staging_ns
      DOCKER_BUILDKIT: "1"

      UPSTREAM: mirumee/saleor
      UPSTREAM_VERSION: master

    steps:
      - uses: actions/checkout@v2
        with:
          submodules: recursive
          token: ${{ secrets.SALEOR_PAT }}

      - uses: actions/checkout@v2
        with:
          repository: mirumee/saleor
          path: core

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v1

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
        with:
          install: true

      - name: Pull from Cache
        uses: actions/cache@v2
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ hashFiles('Dockerfile') }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Prepare Variables
        run: |
          set -x

          deployment_repo="727618530289.dkr.ecr.us-east-1.amazonaws.com/saleor-multitenant"
          core_version=$(sed -n 's/ARG\sVERSION="\(\S*\)"/\1/p' Dockerfile)
          multitenant_hash=$(git rev-parse --short HEAD)
          core_hash=$(git -C core rev-parse --short HEAD)

          image_version="${core_version}-${multitenant_hash}-master-${core_hash}"

          tags="${deployment_repo}:master","${deployment_repo}:${image_version}"

          echo "TAGS=${tags}" >> $GITHUB_ENV
          echo "IMAGE_VERSION=${image_version}" >> $GITHUB_ENV
          echo "IMAGE_TAG=${deployment_repo}:${image_version}" >> $GITHUB_ENV

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build
        uses: docker/build-push-action@v2
        with:
          context: ./
          tags: ${{ env.TAGS }}
          target: plugins
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache
          build-args: |
            UPSTREAM=${{ env.UPSTREAM }}
            VERSION=${{ env.UPSTREAM_VERSION }}
            IMAGE_VERSION=${{ env.IMAGE_VERSION }}
            STATIC_URL=/static/
          # It is *not* possible currently to push a multi-arch image
          # than directly using buildx
          push: true

      - name: Test
        run: |
          docker build . \
            --cache-from type=local,src=/tmp/.buildx-cache \
            --load \
            -t multitenant_api \
            --build-arg UPSTREAM=${UPSTREAM} \
            --build-arg VERSION=${UPSTREAM_VERSION} \
            --build-arg IMAGE_VERSION=${IMAGE_VERSION} \
            --build-arg STATIC_URL=/static/
          COMPOSE_DOCKER_CLI_BUILD=1 docker-compose --project-name multitenant run --rm api pytest

      - name: Deploy to Staging
        run: |
          set -x

          aws eks --region "$AWS_DEFAULT_REGION" update-kubeconfig --name "$DEPLOY_CLUSTER_NAME" --role "$AWS_CD_ROLE_ARN"

          kubectl patch deployment "$MASTER_DEPLOYMENT_NAME" --type='json' -p="[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\": \"$IMAGE_TAG\"}]" -n "$STAGING_K8S_NAMESPACE"
          kubectl patch deployment "$(echo $MASTER_DEPLOYMENT_NAME | sed 's/saleor/celery/')" --type='json' -p="[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\": \"$IMAGE_TAG\"}]" -n "$STAGING_K8S_NAMESPACE"

          kubectl wait --for=condition=ready pod -l app="$MASTER_DEPLOYMENT_NAME" -n "$STAGING_K8S_NAMESPACE" --timeout 10m
          sleep 20

          pod_name=$(kubectl get pod -l app="$MASTER_DEPLOYMENT_NAME" --field-selector=status.phase=Running -o jsonpath="{.items[0].metadata.name}" -n "$STAGING_K8S_NAMESPACE")
          kubectl exec -n "$STAGING_K8S_NAMESPACE" $pod_name -- ./manage.py migrate_schemas

      - name: Notify Slack
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
          WEBHOOK_URL: ${{ secrets.SLACK_ERRORS_STAGING_WEBHOOK_URL }}
        run: |
          if  [[ "$JOB_STATUS" = "success" ]]; then
            body=".github/workflows/slack_webhook_build_success.json"
          else
            body=".github/workflows/slack_webhook_build_failed.json"
          fi

          curl -v -X POST -H "content-type: application/json" --data @"$body" "$WEBHOOK_URL"
